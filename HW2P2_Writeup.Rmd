---
title: "HW2P2_Writeup"
author: "Billy Yuan, Lindsay Tober"
date: "August 15, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Author Attribution
Author attribution is one of the many applications of text analysis. Given a group of documents by various authors, how accurately can we predict the authorship of an out-of-sample document? Reuters, a well-known news source, is a prime subject for conducting author attribution analysis.

```{r P2_setup,include=FALSE}
# Setup
{
  library(tm)
  library(nnet)
  library(klaR)
  library(XML)
  library(ggplot2)
  library(knitr)
  
  # ## BILLY
  # setwd("~/Dropbox/MSBA/Summer 2016/STA380-master")
  # source('./R/textutils.R')
  
  ## LINDSAY
  setwd("~/Documents/MSBA/GitHub/STA380-HW2")
  source('textutils.R')
}

# Data import and text processing
{
  ## Read in files
  {
    # ## BILLY 
    # author_dirs_2 = Sys.glob('./data/ReutersC50/C50train/*') # train
    # author_dirs_test = Sys.glob('./data/ReutersC50/C50test/*') # test
    
    ## LINDSAY
    author_dirs_train = Sys.glob('dataset/ReutersC50/C50train/*') # train
    author_dirs_test = Sys.glob('dataset/ReutersC50/C50test/*') # test
    
    # readerPlain function, used to apply file names
    readerPlain = function(fname){
      readPlain(elem=list(content=readLines(fname)), 
                id=fname, language='en') }
  }
  
  ## Import and process training set
  {
    file_list_train = NULL
    labels_train = NULL
    
    for(author in author_dirs_train) {
      author_name = substring(author, first=29)
      files_to_add = Sys.glob(paste0(author, '/*.txt'))
      file_list_train = append(file_list_train, files_to_add)
      labels_train = append(labels, rep(author_name, length(files_to_add)))
      }
    
    all_docs_train = lapply(file_list_train, readerPlain) 
    names(all_docs_train) = file_list_train
    names(all_docs_train) = sub('.txt', '', names(all_docs_train))
    
    my_corpus_train = Corpus(VectorSource(all_docs_train))
    names(my_corpus_train) = file_list_train
  
    # Preprocessing
    my_corpus_train = tm_map(my_corpus_train, content_transformer(tolower)) # make everything lowercase
    my_corpus_train = tm_map(my_corpus_train, content_transformer(removeNumbers)) # remove numbers
    my_corpus_train = tm_map(my_corpus_train, content_transformer(removePunctuation)) # remove punctuation
    my_corpus_train = tm_map(my_corpus_train, content_transformer(stripWhitespace)) # remove excess white-space
    my_corpus_train = tm_map(my_corpus_train, content_transformer(removeWords), stopwords("SMART")) # remove stopwords
    
    # Document Term Matrix
    DTM_train = DocumentTermMatrix(my_corpus_train)
    X_train = as.matrix(DTM_train)
  }
  
  ## Import and process test set
  {
    file_list_test = NULL
    labels_test = NULL
    
    for(author in author_dirs_test) {
      author_name = substring(author, first=28)
      files_to_add = Sys.glob(paste0(author, '/*.txt'))
      file_list_test = append(file_list_test, files_to_add)
      labels_test = append(labels, rep(author_name, length(files_to_add)))
      }
    
    all_docs_test = lapply(file_list_test, readerPlain) 
    names(all_docs_test) = file_list_test
    names(all_docs_test) = sub('.txt', '', names(all_docs_test))
    
    my_corpus_test = Corpus(VectorSource(all_docs_test))
    names(my_corpus_test) = file_list_test
    
    my_corpus_test = tm_map(my_corpus_test, content_transformer(tolower)) # make everything lowercase
    my_corpus_test = tm_map(my_corpus_test, content_transformer(removeNumbers)) # remove numbers
    my_corpus_test = tm_map(my_corpus_test, content_transformer(removePunctuation)) # remove punctuation
    my_corpus_test = tm_map(my_corpus_test, content_transformer(stripWhitespace)) # remove excess white-space
    my_corpus_test = tm_map(my_corpus_test, content_transformer(removeWords), stopwords("SMART")) # remove stopwords
    
    DTM_test = DocumentTermMatrix(my_corpus_test)
    
    X_test = as.matrix(DTM_test)
  }
}

# Prepare for modeling
{
  ## Create training set for PCA logistic regression 
  {
    train_log = as.data.frame(X_train)
    train_log$target = 0
    train_log$new_word = 0
    
    test_log = as.data.frame(X_test)
    test_log$target = 0
  }
  
  ## Create target variables that correspond to each author
  {
    for (i in 1:50) {
      start_idx = 50*(i-1) + 1
      end_idx = (50*i)
      
      train_log$target[start_idx:end_idx] = i
      test_log$target[start_idx:end_idx] = i
    }
    
    train_log$target = factor(train_log$target)
    test_log$target = factor(test_log$target)
  }
  
  ## Create X and Y training sets
  {
    X.log = as.matrix(train_log[,names(train_log)!='target'])
    y.log = train_log$target
  }
  
  ## Create X and Y test sets
  {
    X.test = as.matrix(test_log[,names(test_log)!='target'])
    y.test = test_log$target
  
    test.matrix = matrix(data=0,nrow=dim(X.log)[1],ncol=dim(X.log)[2]) # test matrix with same columns as train
    colnames(test.matrix) = colnames(X.log)
  }
  
  ## Build function to sapply over test matrix
  {
    match_columns = which(colnames(X.test) %in% colnames(test.matrix))
    match_train = which(colnames(test.matrix) %in% colnames(X.test[,match_columns]))
  
    test.matrix[,match_train] = X.test[,match_columns]
  }
}

```

### Preparing the Data
The Reuters C50 corpus contains articles written by 50 different authors, where each author has 50 'training' articles and 50 'testing' articles.  The training articles were used to build a Document Term Matrix for predicting author attribution of the 50 testing articles.  This required some transformation of the data in pre-processing (e.g., changing case to all lowercase; removing numbers, punctuation, and excess white-space; and eliminating 'stop words' that are common enough in the English language to remove valuable insight for author attribution) to allow for effective text analysis.

Given the training and testing data sets are two separate corpuses, aligning the terms used in their Document Term Matrices to ensure matching dimensions is critical to accurately modeling author attribution. Establishing a single 'umbrella' term list for the two corpuses requred a couple of assumptions.  First, new terms existing in articles from the testing data set (i.e., terms present in the testing articles that are not present in the training articles) would not have any weight indicators to assist in attributing the articles to a given author, so these 'new' terms were excluded from the umbrella term list. Second, terms present in the training articles that were not present in the testing articles were kept in the umbrella term list as they were used in building the model on the training data (e.g., for Naive Bayes, the weight matrix is constructed using the bag of words model, which required usage of all words to accurately build weights).  This resulted in approximately 10,000 irrelevant words for the test data set, but it ensured that the weights of the common words were accurate for a given document and author.

### Model Summary
Two models were selected to predict author attribution of the testing articles: (1) Principal Component Multinomial Logistic Regression and (2) Naive Bayes.

#### Model 1: Principal Component Multinomial Logistic Regression
```{r PCR_model,include=FALSE}

# Run PCR (Principal Component Multinomial Logistic Regression)
{
## Create S-Matrix
{
  pc_author = prcomp(X.log, center=TRUE)
  K = 100
  V = pc_author$rotation[,1:K]
  S = X.log %*% V # S-Matrix
}
  
## Run Multinomial Logistic Regression
{
  # Model
  model_multi = multinom(y.log ~ .,data=as.data.frame(S),MaxNWts=10200)
  
  # Predictions
  predict_log = predict(model_multi,as.matrix(test.matrix)%*%V)
}
  
## Get PCA Results
{
  # Get accuracy
  get_accuracy = function(k){
    trues=0
    for (i in 1:k){
      if (predict_log[i] == y.test[i]) {
        trues=trues+1
      } else {
        trues=trues
      }
    }
    trues/k
  }
  get_accuracy(2500)
  
  # Create testing matrix to get author accuracy
  pcr_results_df = matrix(0,nrow=2500,ncol=2)
  colnames(pcr_results_df) = c("Actual","Predict")
  pcr_results_df[,2] = predict_log
  
  # Fill the results matrix with with actual authors
  for (author in 1:50) {
    start_idx = 50*(author-1) + 1
    end_idx = (50*author)
    
    pcr_results_df[start_idx:end_idx,1] = author
  }
  
  # Test Accuracy Results
  cat("PCR Test Accuracy is",100*sum(pcr_results_df[,1] == pcr_results_df[,2])/dim(pcr_results_df)[1],"%")
  PCR_test_accuracy = sprintf("%.2f %%",100*sum(pcr_results_df[,1] == pcr_results_df[,2])/dim(pcr_results_df)[1])
  PCR_test_accuracy
  
}
}

```
Conducting Principal Compenent Analysis (PCA) on the training articles provided contexts, or loadings, to apply to the Document Term Matrix.  Given each of the 50 authors had 50 articles included in the training set, PCA resulted in 2,500 principal components to capture variance across the 2,500 documents within the training corpus.  With limitations on computational power, the first 100 principal components were selected for running multinomial logistic regression.  This subset of 100 principal components accounted for 45% of the variance and still took a significant time to process.

A multinomial logistic regression model was trained to predict authors on the 100 principal components for all training articles, then applied to the testing articles to predict author identities in the out-of-sample data set. Multinomial logistic regression using PCA resulted in a test accuracy of 65.46%.

#### Model 2: Naive Bayes
```{r NB_model,include=FALSE}
# Run Naive Bayes
{
## Create authors reference
{
  # Create authors list
  authors = NULL
  
  for (item in author_dirs_train) {
    authors = append(authors, substring(item, first=29))
  }
  
  # Create index for authors for Naive Bayes
  authors_index = as.data.frame(authors)
  train_index = c(1:50)
}

## Build 'master' training set for Naive Bayes
{
  # Smoothing factor
  smooth_factor = 1/nrow(X_train)
  
  # Training table  
  X_NB = matrix(data=NA, nrow=50, ncol=dim(X_train)[2])
  
  for (i in 1:50) {
    start_idx = 50*(i-1) + 1
    end_idx = (50*i)
    
    author_train = X_train[start_idx:end_idx,]
    
    X_NB[i,] = colSums(author_train + smooth_factor)
    X_NB[i,] = X_NB[i,]/sum(X_NB[i,])
  }
}

## Build testing matrix
{
  # Create results matrix for testing
  nb_results_df = matrix(0,nrow=2500,ncol=2)
  colnames(nb_results_df) = c("Actual","Predict")
  
  # Fill the results matrix with with actual authors
  for (author in 1:50) {
    start_idx = 50*(author-1) + 1
    end_idx = (50*author)
    
    nb_results_df[start_idx:end_idx,1] = author
  }
}

## Run loop to make predictions
{
  for (doc in 1:2500) {
  results_store = c()
  
  for (author in 1:50) {
    results_store[author] = sum(test.matrix[doc,]*log(X_NB[author,]))
  }
  
  max_prob = max(results_store)
  nb_results_df[doc,2] = which(results_store == max_prob)
  }
}

## Get Naive Bayes results
{
  # Test Accuracy
  cat("Naive Bayes Test Accuracy is",100*sum(nb_results_df[,1] == nb_results_df[,2])/dim(nb_results_df)[1],"%")
  NB_test_accuracy = sprintf("%.2f %%",100*sum(nb_results_df[,1] == nb_results_df[,2])/dim(nb_results_df)[1])
  NB_test_accuracy
}
}

```
For Naive Bayes, the Document Term Matrix was used to create a training table of multinomial probability vectors by author. Each multinomial probability vector was calculated by aggregating the scores by term across documents written by a given author, adding a smoothing factor (1/2500) to ensure non-zero totals, and taking the respective probability for that term against the author's full volume of terms.  The resulting multinomial probability vectors provide a 'bag of words' situational probability of selecting that word at random from within the host of articles written by that author.

Using the training table of multinomial probability vectors by author, log probabilities for each article within the testing data set were calculated under the Naive Bayes model.  Whichever author resulted in the highest sum of log probabilities was taken as the 'predicted' author. The Naive Bayes model resulted in a test accuracy of 55.28%.


### Model Evaluation
Taken at face value, the Naive Bayes model outperformed the Principal Component Multinomial Logistic Regression (PCR) model by almost 10% (64.56 vs. 55.28%, respectively).  However, model accuracy varied by author:

```{r model_accuracy_comparison_table,fig.align='center',echo=FALSE}

# Where do the models make the most errors?
{
  ## Set up author accuracy table
  {
    author_accuracy_table = matrix(0,nrow=50,ncol=3)
    colnames(author_accuracy_table) = c('Author','NB Accuracy','PCR Accuracy')
    author_accuracy_table[,1] = as.vector(authors)
  }
  
  ## Author accuracy - Naive Bayes
  for (author_idx in 1:50){
    author_results_store = nb_results_df[nb_results_df[,2] == author_idx,] 
    author_accuracy_table[author_idx,2] = sum(author_results_store[,1] == author_results_store[,2])/dim(author_results_store)[1]
  }
  
  ## Author accuracy - PCA
  for (author_idx in 1:50){
    author_results_store = nb_results_df[pcr_results_df[,2] == author_idx,] 
    author_accuracy_table[author_idx,3] = sum(author_results_store[,1] == author_results_store[,2])/dim(author_results_store)[1]
  }
  
  ## Complete Author Accuracy Table
  {
    author_accuracy_table[order(author_accuracy_table[,2],decreasing=FALSE),] # Order by NB Accuracy
    author_accuracy_table[order(author_accuracy_table[,3],decreasing=FALSE),] # Order by PCR Accuracy
    format_percent <- function(x) sprintf("%.2f %%",100*as.numeric(x))
    author_accuracy_table_formatted = data.frame(author_accuracy_table[,1], apply(author_accuracy_table[,2:3],2,format_percent))
    # Fix column names
    colnames(author_accuracy_table_formatted) = c('Author','NB Accuracy','PCR Accuracy')
  }
  
  ## Comparison
  model_diff = as.numeric(author_accuracy_table[,2]) - as.numeric(author_accuracy_table[,3])
  NB_beat_PCR = author_accuracy_table[which(model_diff == max(model_diff)),] # NB beats PCR by the most amount
  PCR_beat_NB = author_accuracy_table[which(model_diff == min(model_diff)),] # PCR beats NB by the most amount
}

# Print Table
kable(author_accuracy_table_formatted, caption='Author Prediction Accuracy by Model')
  
```

As the chart shows, Naive Bayes showed the most benefit over PCR for author Martin Wolk, with a test accuracy of 91.18% for NB and 65.39% for PCR. On the flip side, PCR performed best over Naive Bayes for author Sarah Davison, with a test accuracy of 69.70% for PCR and 44.44% for NB. Aside from the biggest gaps in accuracy between the models, the chart comparing model accuracy reveals that some authors  had high success on both models (e.g., Aaron PRessman, Fumiko Fujisaki) or low success on both models (e.g., David Lawder, Jane Macartney). What might be driving these issues?

We can start to assess this question by looking at the plot of authors against the top two Principal Components:
```{r PCR_plot,echo=FALSE,fig.align='center'}
# Create plot of all authors for first 2 loadings, colored by accuracy / difference 
{
  ## Set up data
  {
    author_plot_pca = prcomp((X_NB),center=TRUE)
    dim(author_plot_pca$rotation)
    author_plot_pca_data = as.data.frame(author_plot_pca$x[,1:2])
    author_plot_pca_data['38-46'] = 1
    author_plot_pca_data['38-46'][38,] = 0
    author_plot_pca_data['38-46'][46,] = 0
  }

  ## Create Plot
  {
  pcr_plot = ggplot(author_plot_pca_data,aes(x=author_plot_pca_data$PC1,y=author_plot_pca_data$PC2,label=rownames(author_plot_pca_data))) +
    ggtitle("PC1 & PC2 by Author") +
    labs(x="PC 1", y="PC 2") +
    geom_text(aes(colour=factor(author_plot_pca_data$`38-46`)),size=6) +
    scale_colour_discrete(guide=FALSE)
  }
}

# Show plot 
pcr_plot 
```

```{r 46_38_comparison,include=FALSE}

# 46 incorrectly attributed to 38  
{
  ## number of errors - PCA
  {
    PCA_total_error_46 = sum(pcr_results_df[pcr_results_df[,1] == 46,][,1] != pcr_results_df[pcr_results_df[,1] == 46,][,2])
  
    PCA_error_38_as_46 = dim(pcr_results_df[pcr_results_df[,1] == 46 & pcr_results_df[,2] == 38,])[1]
  
    cat("PCA predicted author 38 (Peter Humphrey) when the correct author was 46 (Tan EeLyn)",sprintf("%.2f %%", 100*(PCA_error_38_as_46/PCA_total_error_46)),"of the time.")
    PCA_38as46_error_percent = sprintf("%.2f %%", 100*(PCA_error_38_as_46/PCA_total_error_46))
    PCA_38as46_error_percent
  }
    
  ## number of errors - NB
  {
  names(X_NB) = colnames(test.matrix) # name the columns of X_NB to the words in corpus
  nb_46 = nb_results_df[nb_results_df[,1] == 46,] 
  table(nb_46[nb_46[,1] != nb_46[,2],][,2]) # count of mistakes by author guess
  
  sort(test.matrix[2261,],decreasing=TRUE)[1:10] # top 10 words used in article 2261, which is
  # an article in which the correct author was 46 and the model guessed 38
  
  top_10_author_46=(sort(X_NB[46,],decreasing=TRUE)[1:10]) # indexes of  10 words used by author 46
  top_10_author_38=(sort(X_NB[38,],decreasing=TRUE)[1:10]) # indexes of top 10 words used by author 38


    # top 10 words used by authors 38 and 46
  top_probs_46 = names(X_NB)[which(X_NB[46,] %in% top_10_author_46)] # top 10 words used by author 46
  top_probs_38 = names(X_NB)[which(X_NB[38,] %in% top_10_author_38)] # top 10 words used by author 38
  }
}
  
```

Authors 38 (Peter Humphrey) and 46 (Tan EeLyn) stand out as two authors  that are closer together in the plane of P1 x P2 but further away from most of the other authors.  Going back to the summary table, we see that Tan EeLyn (author 46) had test accuracies of 39.02% for NB and 38.18% for PCR.  Given the close distance to 38, we can check the portion of errors where test articles for author 46 were incorrectly attributed to author 38.  Out of 30 incorrect attributions for author 46, 15 documents (or 50.00%) were attributed to author 38.

How does this interaction play out within the Naive Bayes model?  For Naive Bayes, 55.88% of the incorrect attributions for author 46 were to author 38.  Similar to the closer loadings of PC1 and PC2 under the PCR model, we can confirm whether authors 46 and 47 have similar weights for the Naive Bayes model.  This is done easily by comparing the top 10 terms from both of authors' multinomial probability vectors:

```{r 46_38_comparison_table,echo=FALSE,fig.align='center'}
# Multinomial probability vector comparison table
{
  top10_46_vs_38 = cbind(top_probs_46, top_probs_38)
  colnames(top10_46_vs_38) = c("Author 46 (Tan EeLyn)", "Authors 38 (Peter Humphrey)")
  kable(top10_46_vs_38, caption='Top 10 Terms of Multinomial Probability Vector')
}
```

Both lists of terms are very similar, which may explain why many of the test articles for author 46 are attributed to author 38.  Take, for example, test article #2261, which had the following term frequencies:

```{r test_article,echo=FALSE,fig.align='center'}
# Test Article 2261
{
  article_2261 = as.data.frame(sort(test.matrix[2261,],decreasing=TRUE)[1:10])
  colnames(article_2261) = c('Frequency in Document')
  kable(article_2261, caption='Top 10 Terms by Frequency')
}
```

Many of these terms appear in the top 10 lists for authors 46 and 38. In addition, the higher frequency of legislature in test article #2261 may have been one of the key drivers for attributing that document to author 46 instead of 38, as 'legislature' has a higher relative multinomial probability for author 46.

While both models significantly improved the chances of correctly attributing the author - up to 64.56 for NB and 55.28% for PCR from 1/50, or 2% at random - Naive Bayes did have a slightly higher test accuracy.  In addition, the results of the Naive Bayes model were easier to interpret, given the 'bag of words' probabilities by term are more intuitive than the contextual representation of Principal Components.  As such, we prefer to use the Naive Bayes model for author attribution.  Given there were only 50 articles from each author in the training data set, we believe that the Naive Bayes model could be significantly improved by including additional training articles to further differentiate multinomial probability vectors between different authors.

</br>
